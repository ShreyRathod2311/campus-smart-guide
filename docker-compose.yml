services:
  # ─────────────────────────────────────────────────────────────────
  # Ollama — local LLM server
  # ─────────────────────────────────────────────────────────────────
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    # Uncomment below for NVIDIA GPU pass-through:
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: [ "CMD-SHELL", "ollama list || exit 1" ]
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 20s

  # ─────────────────────────────────────────────────────────────────
  # Pull required models on first run
  # ─────────────────────────────────────────────────────────────────
  ollama-setup:
    image: curlimages/curl:latest
    container_name: ollama-setup
    depends_on:
      ollama:
        condition: service_healthy
    entrypoint: >
      sh -c "
        echo '=== Pulling llama3.2 (chat model) ===' &&
        curl -sf --max-time 600 http://ollama:11434/api/pull -d '{\"name\":\"llama3.2\"}' &&
        echo '' && echo '=== Pulling nomic-embed-text (embedding model) ===' &&
        curl -sf --max-time 300 http://ollama:11434/api/pull -d '{\"name\":\"nomic-embed-text\"}' &&
        echo '' && echo '=== All models ready! ==='
      "
    restart: "no"

  # ─────────────────────────────────────────────────────────────────
  # Python AI backend (FastAPI + RAG + Image Gen)
  # ─────────────────────────────────────────────────────────────────
  backend:
    build:
      context: ./ai-model
      dockerfile: Dockerfile
    container_name: campus-backend
    ports:
      - "8000:8000"
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_MODEL=llama3.2
      - OLLAMA_EMBEDDING_MODEL=nomic-embed-text
    depends_on:
      ollama:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: [ "CMD-SHELL", "python -c \"import urllib.request; urllib.request.urlopen('http://localhost:8000/')\" || exit 1" ]
      interval: 15s
      timeout: 10s
      retries: 20
      start_period: 60s

  # ─────────────────────────────────────────────────────────────────
  # React frontend (Vite build → nginx)
  # ─────────────────────────────────────────────────────────────────
  frontend:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        - VITE_SUPABASE_URL=${VITE_SUPABASE_URL:-}
        - VITE_SUPABASE_PUBLISHABLE_KEY=${VITE_SUPABASE_PUBLISHABLE_KEY:-}
        - VITE_USE_LOCAL_AI=true
    container_name: campus-frontend
    ports:
      - "3000:80"
    depends_on:
      backend:
        condition: service_started
    restart: unless-stopped

volumes:
  ollama_data:
